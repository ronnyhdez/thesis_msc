# A data driven approach to predict GPP from VIs through machine learning methods

## Introduction

The field of Earth Science has witnessed a transformative shift with the integration of Machine Learning (ML) methods, which had lead to a deeper understanding of our planet's complex ecosystems and processes [@reichstein_deep_2019]. ML methods have being used in recent years and are now well stablished in environmental sciences [@lary_machine_2016], harnessing the power of ML algorithms to explore diverse aspects of Earth's natural systems including research to understand vegetation characteristics and ecological patterns [@lehnert_retrieval_2015; @verrelst_machine_2012]

The increasing number of EC sites [@tramontana_predicting_2016] couple with the continuously growing amount of Earth system data surpassing the dozens of petabytes [@reichstein_deep_2019], has lead to an emergence of purely data-driven methodologies. These approaches have been important to further develop the quantification of global terrestrial photosynthesis [@jung_global_2011; @tramontana_predicting_2016] and have demonstrated remarkable results in the regression estimation of biogeo-physical parameters using remotely sensed reflectance data, both at local and global scales [@coops_prediction_2003; @verrelst_retrieval_2012].

Furthermore, these data-driven approaches have contributed significantly to the scientific community by providing spatial, seasonal, and interannual variations in predicted fluxes. These predictions, generated through machine learning methodologies, are now serving as important benchmarks for evaluating the performance of physical land-surface and climate models [@jung_recent_2010; @bonan_improving_2011; @anav_spatiotemporal_2015].

Some of the differences between data driven models and process-based methods are
the inherent observational character of data driven models and that functional relationships emerge from the patterns found in the data, rather than being stated before [@tramontana_predicting_2016]. So functional relationships between in-situ measured fluxes with the explanatory variables can emerge [@tramontana_predicting_2016]. This paradigm shift toward data-driven modeling to extract patterns represents an opportunity to come up with new ideas and questioning establisehd theories in earth system models.

One of the distinguishing characteristics between data-driven models and process-based methods lies in their fundamental approach. Data driven models inherently possess an observational nature where functional relationships emerge from the patterns found in the data, rather than being predefined, such as the relationships between in-situ measured fluxes and the explanatory variables. [@tramontana_predicting_2016]. This paradigm shift toward data-driven modeling to extract patterns, represents an opportunity to explore novel ideas and question established theories in earth system models [@reichstein_deep_2019].

For instance, the application of spatially explicit global data driven methods, has unveiled discrepancies in the estimation of photosynthesis within tropical rainforests when compared to climate models [@beer_terrestrial_2010]. This overestimation, has led to the creation of hypothesis for a better understanding of radiative transfer in vegetation canopies [@bonan_improving_2011] which can result in better photosynthesis estimates.

To get the most of the Earth system data there are two major challenges. The first involves efficiently extracting valuable insights from the available data. The second is developing models that learn much more from data than traditional data assimilation approaches can, while still aligning with our evolving comprehension of nature's laws [@reichstein_deep_2019].

Predicting dynamics in the biosphere is challenging given the biologically mediated processes [@reichstein_deep_2019]. "Prediction" term should not be confused with forecasting, given that most of the models are not aiming at predicting into the future, instead the focus is to predict in the past or the present times [@meyer_improving_2018]

These predictions can include many forms of uncertainty [@reichstein_deep_2019]. One is that individual ML methods can have different responses, specially when these models are applied beyond the conditions presented in the training dataset. [@jung_towards_2009; @papale_effect_2015]. A second one is that the explanatory variables used in ML methods derived from satellite remote sensing, are partial on the about the vegetation state [@tramontana_predicting_2016], and also lacks all the information to explain the complete variability in fluxes [@tramontana_uncertainty_2015]

\newpage

## Methods

```{r libraries and sources}
#| echo: false
#| message: false
#| warning: false

# Libraries
library(ggplot2)
library(cowplot)
library(lubridate)
library(purrr)
library(broom)
library(gt)
library(tidymodels)
library(broom)
library(usemodels)
library(vip)
library(h2o)
library(stringr)
library(DALEX)
library(DALEXtra)
library(forcats)

# Source files
# Source the objects created for the complete GPP trends.
# This file will source the code and load objects to memory.
source("scripts/trend_plots.R")

# Source the objects created for the complete GPP trends
source("scripts/models_data_preparation.R")

# Source file with functions to plot rf predictions
source("R/plot_exploratory.R")
```

<!--  - Sites -->
<!--  - ONEFlux for GPP -->
<!--  - Satellite imagery -->
<!--  - indices calculation -->
<!--  - Here I'm using all the bands available -->
<!--  - Cleaning satellite data -->
<!--  - Filter -->
<!--  - scaling -->
<!--  - join -->

### Eddy Covariance sites

For this study, we selected three deciduous broadleaf forest forests sites:
University of Michigan Biological Station located in northern Michigan, USA
(45°350 N 84°430 W), Bartlett experimental forest in New Hampshire, USA 
(44°06 N, 71°3 W), and the Borden Forest Research Station (44°19 N, 79°56 W) in
Ontario, Canada. 

In-situ data such as GPP was obtained utilizing the ONEFlux estimation 
processing by Ameriflux. Here, we selected GPP estimation done by the daytime
method [@pastorello2020fluxnet2015] on a daily, weekly, and monthly basis.

These sites were selected to ensure they represented a single ecosystem type,
characterized by shared environmental features. This approach allowed us to
treat the dataset as a representation of a specific vegetation type terrestrial ecosystem.

To capture seasonal variations and long-term trends, GPP data was collected over
a minimum of 2 years. Specifically, University of Michigan Biological Station 
collected data spanned from January 2015 to January 2018, Bartlett experimental
forest data ranges from January 2015 to December 2018, and Borden Forest 
Research Station from January 2015 to January 2022.

<!-- From ameriflux: -->
<!--  https://ameriflux.lbl.gov/sites/siteinfo/US-Bar -->
<!--  https://ameriflux.lbl.gov/sites/siteinfo/CA-Cbo -->
<!--  https://ameriflux.lbl.gov/sites/siteinfo/US-UMB -->

<!-- All sites are Dfb -->

<!-- The code "Dfb" in the Köppen system refers to a specific climate type, which can be described as follows: -->

<!-- "D" stands for the warm-summer continental or hemiboreal climate. -->
<!-- "f" indicates that this climate has significant precipitation in all seasons. -->
<!-- "b" indicates that the warmest month has an average temperature between 22°C and 28°C. -->

### Satellite imagery 

We used Google Earth Engine (GEE) to retrieve data from the Terra Moderate 
Resolution Imaging Spectroradiometer (MODIS), specifically the collection 
MOD09GA Version 6.1 product. A square polygon with an area of 3km surrounding 
the EC tower was defined for each of the study sites, and the complete data 
pixel values within this polygon was extracted for analysis.

The MODIS collected data contains the surface spectral reflectance from bands 
to 1 through 7 with a spatial resolution of 500m, with corrections for 
atmospheric conditions such as aerosols, gasses and Rayleigh scattering [@vermote2021modis].

We selected the highest quality pixels according to the `state_1km` 
(@tbl-state_1km_bitstrings) and `qc_500m` (@tbl-qc_scan_bit_strings) bit 
string variables. Once we had just the highest quality pixels, all the band
values were scaled by a factor of 0.0001. If any value fell outside the range
of 0 to 1 after the scaling, it was discarded.

Once all the band values were scaled, we calculated 4 Vegetation Indices: NDVI,
NIRv, EVI, and CCI. Then all the MODIS bands values and VIs were summarized in
a daily, weekly, and monthly basis to be merge with the GPP values from ONEFlux.

### Random Forests

Regression Random forests were used as an approach to predict GPP by 
incorporating all available bands values (from B01 to B07) from the MODIS
dataset and the calculated VIs. Three distinct models were developed, each one
tailored for a specific time scale (daily, weekly, and monthly). This approach
allowed us to assess the prediction performance of GPP at different time scales. 

To build and evaluate the model, we performed a random data splitting procedure,
dividing the data into training set with 70% of the observations and a test set
with the remaining 30%. We employed a stratified data split to ensure a 
proportional representation of each site category in both sets. To ensure 
reproducibility, we use a random number generator state throughout the process.

To implement the RF models, we use the `ranger` package in R [@wright_ranger_2017],
utilizing 1000 trees within the forest ensemble. The models were trained using
the bootstrap resampling technique with 100 folds, which helps to improve the
robustness and accuracy of the predictions. 

We calculated the variable of importance to understand which MODIS bands or
VIs are driving the predictions in each of the regression random forest models.
To measure the influence of each feature on the model's predictive
performance, we quantify how this performance deteriorates when a particular 
variable is permuted while keeping others constant. 







To understand which features were the most important to predict GPP and how
the predictions values could change under different predictor values, we
obtained the shapley values for each of the models.

VIP is the model based method
DALEX is model agnostic


### AutoML

Corporis cumque voluptate cum fuga consequuntur pariatur. Excepturi perspiciatis omnis dolores dolorum officiis a consequatur. Quae distinctio quae ullam sit id. Quasi minima voluptatibus nihil ut quibusdam aut tempore nam. Repudiandae quasi quis ipsa aut. Temporibus ut rerum ea a est voluptate.

Corporis iusto necessitatibus aut rerum eum. Voluptatem repellendus soluta doloremque. Et reiciendis et animi ut enim. Fugiat consequatur hic laborum culpa blanditiis explicabo nobis quae. Quae pariatur quo et hic autem.

\newpage

## Results

### Data-Driven GPP Prediction: A Regression Random Forest Approach 

```{r data_preparation_rf}
#| echo: false
#| message: false
#| warning: false

# 500
# Dataset to use: daily_500 for all sites
bor <- borden_daily_500 %>% 
  select(ends_with(c("_mean")),
         gpp_dt_vut_ref, total_obs) %>% 
  mutate(site = "borden")

bar <- bartlett_daily_500 %>% 
  select(ends_with(c("_mean")),
         gpp_dt_vut_ref, total_obs) %>% 
  mutate(site = "bartlett")

mich <- michigan_daily_500 %>% 
  select(ends_with(c("_mean")),
         gpp_dt_vut_ref, total_obs) %>% 
  mutate(site = "michigan")

daily_500_rf <- bind_rows(bor, bar, mich) %>% 
  select(-kndvi_mean)

# Dataset to use: weekly_500 for all sites
bor <- borden_weekly_500 %>% 
  select(ends_with(c("_mean")),
         gpp_dt_vut_ref, total_obs) %>% 
  mutate(site = "borden")

variables <- names(bor)

bar <- bartlett_weekly_500 %>% 
  mutate(site = "bartlett") %>% 
  select(all_of(variables))

mich <- michigan_weekly_500 %>% 
  mutate(site = "michigan") %>% 
  select(all_of(variables))

weekly_500_rf <- bind_rows(bor, bar, mich) %>% 
  select(-kndvi_mean)

## Dataset to use: monthly_500 for all sites
bor <- borden_monthly_500 %>% 
  select(ends_with(c("_mean")),
         gpp_dt_vut_ref, total_obs) %>% 
  mutate(site = "borden")

variables <- names(bor)

bar <- bartlett_monthly_500 %>% 
  mutate(site = "bartlett") %>% 
  select(all_of(variables))

mich <- michigan_monthly_500 %>% 
  mutate(site = "michigan") %>% 
  select(all_of(variables))

monthly_500_rf <- bind_rows(bor, bar, mich) %>% 
  select(-kndvi_mean)
```

#### Daily 500

```{r xgboost_daily}
#| echo: false
#| message: false
#| warning: false
# set.seed(123)
# daily_500_split <- initial_split(daily_500_rf, strata = site)
# daily_500_train <- training(daily_500_split)
# daily_500_test <- testing(daily_500_split)
# 
# set.seed(234)
# # daily_500_folds 
# daily_500_folds <- bootstraps(daily_500_train,
#                               times = 100,
#                               strata = gpp_dt_vut_ref)
# 
# bb_recipe <- 
#   recipe(formula = gpp_dt_vut_ref ~ ., data = daily_500_train) %>% 
#   step_select(-site, -total_obs)
# 
# xgb_spec <-
#   boost_tree(
#     trees = tune(),
#     min_n = tune(),
#     mtry = tune(),
#     learn_rate = 0.01
#   ) %>%
#   set_engine("xgboost") %>%
#   set_mode("regression")
# 
# xgb_wf <- workflow(bb_recipe, xgb_spec)
# 
# set.seed(3156)
# 
# library(finetune)
# doParallel::registerDoParallel()
# 
# set.seed(345)
# xgb_rs <- tune_race_anova(
#   xgb_wf,
#   resamples = daily_500_folds,
#   grid = 15,
#   # metrics = metric_set(mn_log_loss),
#   control = control_race(verbose_elim = TRUE)
# )
# 
# plot_race(xgb_rs)
# 
# show_best(xgb_rs)
# 
# xgb_last <- xgb_wf %>%
#   finalize_workflow(select_best(xgb_rs)) %>%
#   last_fit(daily_500_split)
# 
# xgb_last
# metrics <- collect_metrics(xgb_last)
# 
# plot_predictions_rf(xgb_last, metrics, 4, 4, 23.5, 22) 
```


```{r daily_500_rf}
#| echo: false
#| message: false
#| warning: false

## Make sure that the source of the file "R/plot_exploratory.R" was succesful

set.seed(752)
daily_500_split <- initial_split(daily_500_rf, strata = site)
daily_500_train <- training(daily_500_split)
daily_500_test <- testing(daily_500_split)

set.seed(234)
# daily_500_folds 
daily_500_folds <- bootstraps(daily_500_train,
                              times = 100,
                              strata = gpp_dt_vut_ref)

ranger_recipe <- 
  recipe(formula = gpp_dt_vut_ref ~ ., data = daily_500_train) %>% 
  step_select(-site, -total_obs, skip = TRUE)

ranger_spec <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_mode("regression") %>% 
  set_engine("ranger") 

ranger_workflow <- 
  workflow() %>% 
  add_recipe(ranger_recipe) %>% 
  add_model(ranger_spec) 

# Conditional to re-run model if no artifac was saved before.
if (fs::file_exists("models/daily_500_fit_site.rds") & 
    fs::file_exists("models/daily_500_site_ranger_tune.rds")) {
  
  daily_500_fit <- readRDS("models/daily_500_fit_site.rds")
  ranger_tune <- readRDS("models/daily_500_site_ranger_tune.rds")
  
} else {  
  doParallel::registerDoParallel()
  set.seed(6578)
  ranger_tune <-
    tune_grid(ranger_workflow, 
              resamples = daily_500_folds, 
              grid = 12)
  
  # Final fit
  final_rf <- ranger_workflow %>% 
    finalize_workflow(select_best(ranger_tune))
  
  daily_500_fit <- last_fit(final_rf, daily_500_split)
  
  ## last_fit is saved if no model has been trained and saved.
  saveRDS(ranger_tune, "models/daily_500_site_ranger_tune.rds")
  saveRDS(daily_500_fit, "models/daily_500_fit_site.rds")
}
```


```{r predictions_plot_daily_500_rf}
#| label: fig-daily_500_rf
#| fig-cap: GPP observed and predicted values from the Random Forest model for all the sites at a daily basis. The red line represents a 1:1 relation.
#| echo: false
#| message: false
#| warning: false

# Explore RF results
## Check the metrics
metrics <- collect_metrics(daily_500_fit) 

## Collect predictions
plot_predictions_rf(daily_500_fit, metrics, 4, 4, 23.5, 22) 
```


```{r vip_plot_daily_500_rf}
#| label: fig-vip_daily_500_rf
#| fig-cap: "Variable of importance derived from the Random forest model for the daily values at 500 m spatial resolution model."
#| echo: false
#| message: false
#| warning: false

## Feature importance
imp_spec <- ranger_spec %>%
  finalize_model(select_best(ranger_tune)) %>%
  set_engine("ranger", importance = "permutation")

workflow() %>%
  add_recipe(ranger_recipe) %>%
  add_model(imp_spec) %>%
  fit(daily_500_train) %>%
  extract_fit_parsnip() %>% 
  vip(aesthetics = list(alpha = 0.8, fill = "midnightblue")) +
  theme_light(base_size = 12) +
  scale_x_discrete(labels = c("ndvi_mean" = "NDVI",
                              "nirv_mean" = "NIRv",
                              "evi_mean" = "EVI",
                              "cci_mean" = "CCI",
                              "sur_refl_b01_mean" = "B01",
                              "sur_refl_b02_mean" = "B02",
                              "sur_refl_b03_mean" = "B03",
                              "sur_refl_b04_mean" = "B04",
                              "sur_refl_b05_mean" = "B05",
                              "sur_refl_b06_mean" = "B06",
                              "sur_refl_b07_mean" = "B07"))
```

```{r}
#| label: fig-shap_daily_500_rf
#| fig-cap: "Shapley values derived from the Random forest model for the daily values at 500 m spatial resolution model."
#| echo: false
#| message: false
#| warning: false

# Extract workflow to obtain shapley values (or run predict)
daily_gpp_model <- extract_workflow(daily_500_fit)

# Create an explainer for a regression model
explainer_rf <- explain_tidymodels(
  daily_gpp_model,
  data = daily_500_train,
  y = daily_500_train$gpp_dt_vut_ref,
  label = "rf",
  verbose = FALSE
)

# Take a low gpp value
low_gpp <- daily_500_train[71, ] 

rf_breakdown <- predict_parts(explainer = explainer_rf, 
                              new_observation = low_gpp,
                              type = "shap")
# rf_breakdown
low_value <- rf_breakdown %>%
  group_by(variable) %>%
  mutate(mean_val = mean(contribution)) %>%
  ungroup() %>%
  mutate(variable = case_when(
    str_detect(variable, "_mean") ~ str_remove(variable, "_mean"),
    .default = variable
  )) %>% 
  mutate(variable = fct_reorder(variable, abs(mean_val))) %>%
  ggplot(aes(contribution, variable, fill = mean_val > 0)) +
  geom_col(data = ~distinct(., variable, mean_val), 
           aes(mean_val, variable), 
           alpha = 0.5) +
  geom_boxplot(width = 0.5) +
  theme_light() +
  theme(legend.position = "none") +
  scale_fill_viridis_d() +
  labs(y = NULL)

# Take a high gpp value
high_gpp <- daily_500_train[578, ] 

rf_breakdown <- predict_parts(explainer = explainer_rf, 
                              new_observation = high_gpp,
                              type = "shap")
# rf_breakdown
high_value <- rf_breakdown %>%
  group_by(variable) %>%
  mutate(mean_val = mean(contribution)) %>%
  ungroup() %>%
  mutate(variable = case_when(
    str_detect(variable, "_mean") ~ str_remove(variable, "_mean"),
    .default = variable
  )) %>% 
  mutate(variable = fct_reorder(variable, abs(mean_val))) %>%
  ggplot(aes(contribution, variable, fill = mean_val > 0)) +
  geom_col(data = ~distinct(., variable, mean_val), 
           aes(mean_val, variable), 
           alpha = 0.5) +
  geom_boxplot(width = 0.5) +
  theme_light() +
  theme(legend.position = "none") +
  scale_fill_viridis_d() +
  labs(y = NULL)

plot_grid(low_value,
          high_value,
          nrow = 1,
          labels = c('A', 'B'))
```

#### Weekly 500

```{r weekly_500_rf}
#| echo: false
#| message: false
#| warning: false
set.seed(125)

weekly_500_split <- initial_split(weekly_500_rf, strata = site)
weekly_500_train <- training(weekly_500_split)
weekly_500_test <- testing(weekly_500_split)

set.seed(2389)
weekly_500_folds <- bootstraps(weekly_500_train,
                              times = 100,
                              strata = gpp_dt_vut_ref)
ranger_recipe <- 
  recipe(formula = gpp_dt_vut_ref ~ ., data = weekly_500_train) %>% 
  step_select(-site, -total_obs, skip = TRUE)

ranger_spec <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_mode("regression") %>% 
  set_engine("ranger") 

ranger_workflow <- 
  workflow() %>% 
  add_recipe(ranger_recipe) %>% 
  add_model(ranger_spec) 

# Conditional to re-run model if no artifac was saved before.
if (fs::file_exists("models/weekly_500_fit_site.rds")) {
  weekly_500_fit <- readRDS("models/weekly_500_fit_site.rds")
  ranger_tune <- readRDS("models/weekly_500_site_ranger_tune.rds")
} else {
  doParallel::registerDoParallel()
  set.seed(1297)
  ranger_tune <-
    tune_grid(ranger_workflow, 
              resamples = weekly_500_folds, 
              grid = 12)
  
  # Final fit
  final_rf <- ranger_workflow %>% 
    finalize_workflow(select_best(ranger_tune))
  
  weekly_500_fit <- last_fit(final_rf, weekly_500_split)
  
  ## last_fit is saved if no model has been trained and saved.
  saveRDS(ranger_tune, "models/weekly_500_site_ranger_tune.rds")
  saveRDS(weekly_500_fit, "models/weekly_500_fit_site.rds")
}
```

```{r predictions_plot_weekly_500_rf}
#| label: fig-weekly_500_rf
#| fig-cap: "GPP observed and predicted values from the Random Forest for all the sites at a weekly basis. The red line represents a 1:1 relation."
#| echo: false
#| message: false
#| warning: false

# Explore RF results
## Check the metrics
metrics <- collect_metrics(weekly_500_fit) 

## Collect predictions
plot_predictions_rf(weekly_500_fit, metrics, 4, 4, 23, 25)
```

```{r vip_plot_weekly_500_rf}
#| label: fig-vip_weekly_500_rf
#| fig-cap: "Variable of importance derived from the Random forest model for the weekly values at 500 m spatial resolution model."
#| echo: false
#| message: false
#| warning: false

## Feature importance
imp_spec <- ranger_spec %>%
  finalize_model(select_best(ranger_tune)) %>%
  set_engine("ranger", importance = "permutation")

workflow() %>%
  add_recipe(ranger_recipe) %>%
  add_model(imp_spec) %>%
  fit(weekly_500_train) %>%
  extract_fit_parsnip() %>%
  vip(aesthetics = list(alpha = 0.8, fill = "midnightblue")) +
  theme_classic(base_size = 12)
```

```{r shapley_values_weekly}
#| label: fig-shap_weekly_500_rf
#| fig-cap: "Shapley values derived from the Random forest model for the weekly values at 500 m spatial resolution model."
#| echo: false
#| message: false
#| warning: false

# Extract workflow to obtain shapley values (or run predict)
weekly_gpp_model <- extract_workflow(weekly_500_fit)

# Create an explainer for a regression model
explainer_rf <- explain_tidymodels(
  weekly_gpp_model,
  data = weekly_500_train,
  y = weekly_500_train$gpp_dt_vut_ref,
  label = "rf",
  verbose = FALSE
)

# Take a low gpp value
low_gpp <- weekly_500_train[16, ] 

rf_breakdown <- predict_parts(explainer = explainer_rf, 
                              new_observation = low_gpp,
                              type = "shap")
# rf_breakdown
low_value <- rf_breakdown %>%
  group_by(variable) %>%
  mutate(mean_val = mean(contribution)) %>%
  ungroup() %>%
  mutate(variable = case_when(
    str_detect(variable, "_mean") ~ str_remove(variable, "_mean"),
    .default = variable
  )) %>% 
  mutate(variable = fct_reorder(variable, abs(mean_val))) %>%
  ggplot(aes(contribution, variable, fill = mean_val > 0)) +
  geom_col(data = ~distinct(., variable, mean_val), 
           aes(mean_val, variable), 
           alpha = 0.5) +
  geom_boxplot(width = 0.5) +
  theme_light() +
  theme(legend.position = "none") +
  scale_fill_viridis_d() +
  labs(y = NULL)

# Take a high gpp value
high_gpp <- weekly_500_train[167, ] 

rf_breakdown <- predict_parts(explainer = explainer_rf, 
                              new_observation = high_gpp,
                              type = "shap")
# rf_breakdown
high_value <- rf_breakdown %>%
  group_by(variable) %>%
  mutate(mean_val = mean(contribution)) %>%
  ungroup() %>%
  mutate(variable = case_when(
    str_detect(variable, "_mean") ~ str_remove(variable, "_mean"),
    .default = variable
  )) %>% 
  mutate(variable = fct_reorder(variable, abs(mean_val))) %>%
  ggplot(aes(contribution, variable, fill = mean_val > 0)) +
  geom_col(data = ~distinct(., variable, mean_val), 
           aes(mean_val, variable), 
           alpha = 0.5) +
  geom_boxplot(width = 0.5) +
  theme_light() +
  theme(legend.position = "none") +
  scale_fill_viridis_d() +
  labs(y = NULL)

plot_grid(low_value,
          high_value,
          nrow = 1,
          labels = c('A', 'B'))
```

#### Monthly

```{r monthly_500_rf}
#| echo: false
#| message: false
#| warning: false
set.seed(973)

monthly_500_split <- initial_split(monthly_500_rf, strata = site)
monthly_500_train <- training(monthly_500_split)
monthly_500_test <- testing(monthly_500_split)

# monthly_500_folds 
set.seed(365)
monthly_500_folds <- bootstraps(monthly_500_train,
                                times = 100,
                                strata = gpp_dt_vut_ref)

ranger_recipe <- 
  recipe(formula = gpp_dt_vut_ref ~ ., data = monthly_500_train) %>% 
  step_select(-site, -total_obs, skip = TRUE)

ranger_spec <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_mode("regression") %>% 
  set_engine("ranger") 

ranger_workflow <- 
  workflow() %>% 
  add_recipe(ranger_recipe) %>% 
  add_model(ranger_spec) 

# Conditional to re-run model if no artifac was saved before.
if (fs::file_exists("models/monthly_500_fit_site.rds")) {
  monthly_500_fit <- readRDS("models/monthly_500_fit_site.rds")
  ranger_tune <- readRDS("models/monthly_500_site_ranger_tune.rds")
} else {
  set.seed(3159)
  
  doParallel::registerDoParallel()
  ranger_tune <-
    tune_grid(ranger_workflow, 
              resamples = monthly_500_folds, 
              grid = 12)
  
  # Final fit
  final_rf <- ranger_workflow %>% 
    finalize_workflow(select_best(ranger_tune))
  
  monthly_500_fit <- last_fit(final_rf, monthly_500_split)
  
  ## last_fit is saved if no model has been trained and saved.
  saveRDS(ranger_tune, "models/monthly_500_site_ranger_tune.rds")
  saveRDS(monthly_500_fit, "models/monthly_500_fit_site.rds")
}
```

```{r predictions_plot_monthly_500_rf}
#| label: fig-monthly_500_rf
#| fig-cap: "GPP observed and predicted values from the Random Forest for all the sites at a monthly basis. The red line represents a 1:1 relation."
#| echo: false
#| message: false
#| warning: false

# Explore RF results
## Check the metrics
metrics <- collect_metrics(monthly_500_fit) 

## Collect predictions
plot_predictions_rf(monthly_500_fit, metrics, 5, 5, 16, 18)
```

```{r vip_plot_monthly_500_rf}
#| label: fig-vip_monthly_500_rf
#| fig-cap: "Variable of importance derived from the Random forest model for the monthly values at 500 m spatial resolution model."
#| echo: false
#| message: false
#| warning: false

## Feature importance
imp_spec <- ranger_spec %>%
  finalize_model(select_best(ranger_tune)) %>%
  set_engine("ranger", importance = "permutation")

workflow() %>%
  add_recipe(ranger_recipe) %>%
  add_model(imp_spec) %>%
  fit(monthly_500_train) %>%
  extract_fit_parsnip() %>%
  vip(aesthetics = list(alpha = 0.8, fill = "midnightblue")) +
  theme_light(base_size = 12)
```

```{r shapley_values_monthly}
#| label: fig-shap_monthly_500_rf
#| fig-cap: "Shapley values derived from the Random forest model for the monthly values at 500 m spatial resolution model."
#| echo: false
#| message: false
#| warning: false

# Extract workflow to obtain shapley values (or run predict)
monthly_gpp_model <- extract_workflow(monthly_500_fit)

# Create an explainer for a regression model
explainer_rf <- explain_tidymodels(
  monthly_gpp_model,
  data = monthly_500_train,
  y = monthly_500_train$gpp_dt_vut_ref,
  label = "rf",
  verbose = FALSE
)

# Take a low gpp value
low_gpp <- monthly_500_train[45, ] 

rf_breakdown <- predict_parts(explainer = explainer_rf, 
                              new_observation = low_gpp,
                              type = "shap")
# rf_breakdown
low_value <- rf_breakdown %>%
  group_by(variable) %>%
  mutate(mean_val = mean(contribution)) %>%
  ungroup() %>%
  mutate(variable = case_when(
    str_detect(variable, "_mean") ~ str_remove(variable, "_mean"),
    .default = variable
  )) %>% 
  mutate(variable = fct_reorder(variable, abs(mean_val))) %>%
  ggplot(aes(contribution, variable, fill = mean_val > 0)) +
  geom_col(data = ~distinct(., variable, mean_val), 
           aes(mean_val, variable), 
           alpha = 0.5) +
  geom_boxplot(width = 0.5) +
  theme_light() +
  theme(legend.position = "none") +
  scale_fill_viridis_d() +
  labs(y = NULL)

# Take a high gpp value
high_gpp <- monthly_500_train[6, ] 

rf_breakdown <- predict_parts(explainer = explainer_rf, 
                              new_observation = high_gpp,
                              type = "shap")
# rf_breakdown
high_value <- rf_breakdown %>%
  group_by(variable) %>%
  mutate(mean_val = mean(contribution)) %>%
  ungroup() %>%
  mutate(variable = case_when(
    str_detect(variable, "_mean") ~ str_remove(variable, "_mean"),
    .default = variable
  )) %>% 
  mutate(variable = fct_reorder(variable, abs(mean_val))) %>%
  ggplot(aes(contribution, variable, fill = mean_val > 0)) +
  geom_col(data = ~distinct(., variable, mean_val), 
           aes(mean_val, variable), 
           alpha = 0.5) +
  geom_boxplot(width = 0.5) +
  theme_light() +
  theme(legend.position = "none") +
  scale_fill_viridis_d() +
  labs(y = NULL)

plot_grid(low_value,
          high_value,
          nrow = 1,
          labels = c('A', 'B'))
```







\newpage

### The potential of AutoML approaches for GPP predictions

#### Daily autoML

```{r}
#| label: fig-predictions_automl_500
#| fig-cap: "Variable of importance derived from the autoML model for the daily values at 500 m spatial resolution model."
#| echo: false
#| message: false
#| warning: false

predictions_automl <- readRDS("models/predictions_automl.rds")
perf <- readRDS("models/performance_automl.rds")

rsq <- h2o.r2(perf)
rmse <- h2o.rmse(perf)

plot_predictions_automl(predictions_automl, rmse, rsq, 3, 3, 18, 20)
```


```{r automl_importance_variable}
#| label: fig-vip_daily_500_automl
#| fig-cap: "Variable of importance derived from the autoML model for the daily values at 500 m spatial resolution model."
#| fig-width: 7
#| fig-height: 8
#| echo: false
#| message: false
#| warning: false

readRDS("models/automl_va_plot.rds") +
  theme_classic(base_size = 12) +
  scale_fill_viridis_c(direction = -1) +
  labs(title = NULL, x = "Model ID") +
  theme(axis.text.x = element_text(angle = 55, h = 1))
```

#### Weekly autoML


```{r}
#| label: fig-predictions_automl_weekly_500
#| fig-cap: "Variable of importance derived from the autoML model for the weekly values at 500 m spatial resolution model."
#| echo: false
#| message: false
#| warning: false

predictions_automl <- readRDS("models/predictions_automl_weekly.rds")
perf <- readRDS("models/performance_automl_weekly.rds")

rsq <- h2o.r2(perf)
rmse <- h2o.rmse(perf)

plot_predictions_automl(predictions_automl, rmse, rsq, 4, 4, 18, 20)
```


```{r automl_importance_variable_weekly}
#| label: fig-vip_daily_500_automl_weekly
#| fig-cap: "Variable of importance derived from the autoML model for the weekly values at 500 m spatial resolution model."
#| fig-width: 7
#| fig-height: 8
#| echo: false
#| message: false
#| warning: false

readRDS("models/automl_va_plot_weekly.rds") +
  theme_classic(base_size = 12) +
  scale_fill_viridis_c(direction = -1) +
  labs(title = NULL, x = "Model ID") +
  theme(axis.text.x = element_text(angle = 55, h = 1))
```

#### Monthly autoML

```{r}
#| label: fig-predictions_automl_monthly_500
#| fig-cap: "Variable of importance derived from the autoML model for the monthly values at 500 m spatial resolution model."
#| echo: false
#| message: false
#| warning: false
predictions_automl <- readRDS("models/predictions_automl_monthly.rds")
perf <- readRDS("models/performance_automl_monthly.rds")

rsq <- h2o.r2(perf)
rmse <- h2o.rmse(perf)

plot_predictions_automl(predictions_automl, rmse, rsq, 5, 5, 12, 13)
```

```{r automl_importance_variable_monthly}
#| label: fig-vip_monthly_500_automl_monthly
#| fig-cap: "Variable of importance derived from the autoML model for the monthly values at 500 m spatial resolution model."
#| fig-width: 7
#| fig-height: 8
#| echo: false
#| message: false
#| warning: false
readRDS("models/automl_va_plot_monthly.rds") +
  theme_classic(base_size = 12) +
  scale_fill_viridis_c(direction = -1) +
  labs(title = NULL, x = "Model ID") +
  theme(axis.text.x = element_text(angle = 55, h = 1))
```


\newpage

## Discussion


Corporis cumque voluptate cum fuga consequuntur pariatur. Excepturi perspiciatis omnis dolores dolorum officiis a consequatur. Quae distinctio quae ullam sit id. Quasi minima voluptatibus nihil ut quibusdam aut tempore nam. Repudiandae quasi quis ipsa aut. Temporibus ut rerum ea a est voluptate.

Corporis iusto necessitatibus aut rerum eum. Voluptatem repellendus soluta doloremque. Et reiciendis et animi ut enim. Fugiat consequatur hic laborum culpa blanditiis explicabo nobis quae. Quae pariatur quo et hic autem.

Sunt velit eos repellat inventore quia sunt. Et optio aut distinctio non expedita nulla sint. Explicabo sint tempore est in sunt dolores et. Modi sint earum veniam perspiciatis. Velit at beatae nam fugiat iusto.

Sed velit ipsum in qui expedita praesentium. Neque vero optio qui cumque sint. Error possimus dolor quisquam vero aut.

Autem non labore numquam. Eveniet facere id qui impedit. Sunt temporibus sit neque fugiat. Consequatur sit maiores dolorum libero provident a laudantium. Ipsum sit fugiat quis consectetur sed a provident veritatis. Consequuntur laudantium aut libero facere animi ipsum et hic.

\newpage

## References

**Just the references of this chapter, that also needs to be included in the
final references.**

::: {#refs}
:::
